\documentclass[pdftex]{beamer}
%\input{vc}
% 1.77778 is the ratio of 16 to 9
%\setlength{\paperheight}{2.5in} % going way small!
%\setlength{\paperwidth}{1.77778\paperheight}
% 1.33333 is the ratio of 4 to 3
\setlength{\paperheight}{3.0in} % way small!
\setlength{\paperwidth}{1.33333\paperheight}
\setlength{\textwidth}{0.85\paperwidth}
% import the next thing *after* the papersize
\input{./hogg_presentation} % hogg standard colors

\title{Flexible models for noise, variability, and systematics}
\author[David W. Hogg (NYU)]{David W. Hogg \textsl{(NYU \& MPIA)}\\[1ex]
  \textsl{\footnotesize
    in collaboration with:
    Dan~Foreman-Mackey~\textsl{(UW)},
    Bernhard~Sch\"olkopf~\textsl{(MPI-IS)},
    Dun~Wang~\textsl{(NYU)}}}
\date{\textsl{\#eprv} / 2015 July 07}

\newcommand{\conclusions}{%
\begin{frame}
  \frametitle{summary}
  \begin{itemize}
  \item Generalizations of chi-squared fitting can account for
    systematics and correlated noise.
  \item It is important to fit the noise simultaneously and
    marginalize it out (thanks to Bayes).
  \item Make heavy use of Gaussians---and don't be afraid of huge models.
  \item (The science data often deliver housekeeping meta data at
    higher fidelity than any sensors.)
  \end{itemize}
\end{frame}}

\begin{document}

\begin{frame}
  \titlepage
\end{frame}

\conclusions

\begin{frame}
  \frametitle{RV?}
  \begin{itemize}
  \item I am going to talk about \kepler.
  \item I hope the parallels to RV work are clear.
  \item Don't be shy about asking.
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{reminder: $\chi^2$}
  \begin{itemize}
  \item $\chi^2$ is related to the ln of a (Gaussian) likelihood.
  \item It can be written as an inner product (with the noise inverse
    covariance as metric).
  \item If we are going to play with the noise (and we are), we must
    include a $\ln(\det)$ term.
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{reminder: $\chi^2$}
  \begin{eqnarray}
    \chi^2 &\equiv& \sum_i \frac{[y_i - m_i]^2}{\sigma_i^2}
    \nonumber \\
    \chi^2 &=& \transpose{[y - m]}\cdot V^{-1}\cdot [y - m]
    \nonumber \\
    V &=& C
    \nonumber \\
    C_{ij} &\equiv& \sigma_i^2\,\delta_{ij}
    \nonumber \\
    -2\,\ln L &=& \transpose{[y - m]}\cdot V^{-1}\cdot [y - m] + \ln\det V
    \nonumber
  \end{eqnarray}
\end{frame}

\begin{frame}
  \frametitle{reminder: $\chi^2$}
  \begin{itemize}
  \item $\transpose{[y - m]}\cdot V^{-1}\cdot [y - m] + \ln\det V$
  \item $y$ is your data, $m$ is your model (of an exoplanet signal!).
  \item $V$ encodes your beliefs about your (Gaussian) noise.
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Generalizing (Gaussian) noise}
  \begin{itemize}
  \item If there are multiple sources of (Gaussian) noise, their
    covariances add.
  \item Adding new components to the noise variance tensor $V$ is
    equivalent to fitting for other noise processes and marginalizing
    them out.
  \item This is literally as trivial as it sounds.
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Generalizing (Gaussian) noise}
  \begin{eqnarray}
    -2\,\ln L &=& \transpose{[y - m]}\cdot V^{-1}\cdot [y - m] + \ln\det V
    \nonumber \\
    V &=& C + Q
    \nonumber \\
    C_{ij} &\equiv& \sigma_i^2\,\delta_{ij}
    \nonumber \\
    Q &=& \mbox{variance tensor of additional noise}
    \nonumber
  \end{eqnarray}
\end{frame}

\begin{frame}
  \frametitle{Causal idea}
  \begin{itemize}
  \item If a star's behavior can be predicted confidently by
    other stars, then that behavior must be being imprinted by
    the spacecraft.
  \item We are using these ideas to separate spacecraft-induced from
    intrinsic stellar variability {\footnotesize (Wang \etal, in prep;
    Sch\"olkopf \etal, 1505.03036)}.
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Data-driven idea}
  \begin{itemize}
  \item The best predictors of a star (or pixel's) behavior are the
    empirical behaviors of \emph{other stars} (or pixels).
  \item The data \emph{are} the model.
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Data-driven idea}
  \begin{itemize}
  \item Imagine that spacecraft pointing is the source of your
    dominant systematic.
  \item Imagine that you have light curves that are good to a part in
    $10^5$, every one of which depends differently on pointing.
  \item The light curves themselves contain the pointing information
    at very high fidelity.
  \item (Much higher fidelity than any measured centroids!)
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Systematic noise model}
  \begin{itemize}
  \item Imagine that we believe that the spacecraft systematics
    (noise) can be modeled as a sum of $M$ basis vectors.
  \item If we can put a Gaussian prior on the linear amplitudes, we
    can fully represent this noise by adding in a rank-$M$ covariance
    matrix.
  \item This is equivalent to fitting and marginalizing!
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Systematic noise model}
  \begin{eqnarray}
    -2\,\ln L &=& \transpose{[y - m]}\cdot V^{-1}\cdot [y - m] + \ln\det V
    \nonumber \\
    V &=& C + B\cdot\Lambda\cdot\transpose{B}
    \nonumber \\
    B &=& \mbox{block of $M$ basis vectors}
    \nonumber \\
    \Lambda &=& \mbox{$M\times M$ prior variance; can $\rightarrow\infty$}
    \nonumber
  \end{eqnarray}
\end{frame}

\begin{frame}
  \frametitle{Systematic noise model}
  \begin{itemize}
  \item $y$ is your data, $m$ is your model (of an exoplanet signal!).
  \item $V + B\cdot\Lambda\cdot\transpose{B}$ encodes your beliefs
    about your (Gaussian) noise, which now has a systematics
    component.
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Systematic noise {\footnotesize (Foreman-Mackey \etal, 1502.04715)}}
  ~\hfill
  \includegraphics<1>[trim=100 100 100 100, clip, height=0.8\figureheight]{brownbag/brownbagp10.pdf}
  \includegraphics<2>[trim=100 100 100 100, clip, height=0.8\figureheight]{brownbag/brownbagp14.pdf}
  \includegraphics<3>[trim=100 100 100 100, clip, height=0.8\figureheight]{brownbag/brownbagp15.pdf}
  \includegraphics<4>[trim=100 100 100 100, clip, height=0.8\figureheight]{brownbag/brownbagp17.pdf}
\end{frame}

\begin{frame}
  \frametitle{Systematic noise model}
  \begin{itemize}
  \item How do we set $\Lambda$?
  \item Learn it:  Maximum likelihood or Bayes.
  \item (After all, we have a likelihood function!)
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Implementation details}
  \begin{itemize}
  \item For expressions like $C + B\cdot\Lambda\cdot\transpose{B}$
    there are matrix inversion and determinant lemmas.
  \item Never use \code{inv()}. Always use \code{solve()}.
  \item Never use \code{det()}. Always use \code{slogdet()}.
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Systematic noise model}
  \begin{itemize}
  \item Don't ``fit and subtract'' your systematics!
  \item That isn't conservative.
  \item Signals get distorted {\footnotesize (Foreman-Mackey \etal, 1502.04715)}
  \item You need to marginalize out the noise.
  \item Using $C + B\cdot\Lambda\cdot\transpose{B}$ in $\ln L$ is
    equivalent to fitting the noise simultaneously with your model $m$
    and then marginalizing it out!
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Enormous models}
  \begin{itemize}
  \item Don't be afraid of enormous models.
  \item We sometimes use of order $10^9$ nuisance parameters for a
    single \kepler\ lightcurve (70,000 points) {\footnotesize (Wang
      \etal, in prep)}.
  \item In general, for nuisances, you want to use very flexible
    models but to learn the ``priors''\footnote{They are not really
      priors if you learn them!}.
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Gaussians are miraculous}
  \begin{itemize}
  \item ~[that is all]
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Stellar variability noise model}
  \begin{itemize}
  \item Imagine that we believe that stars vary stochastically but
    continuously (\ie, with time correlations).
  \item For any model of the amplitude and covariance of the
    variations, we can construct a covariance matrix (to add in).
  \item This is equivalent to fitting and marginalizing!
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Stellar variability noise model}
  \begin{eqnarray}
    -2\,\ln L &=& \transpose{[y - m]}\cdot V^{-1}\cdot [y - m] + \ln\det V
    \nonumber \\
    V &=& C + K
    \nonumber \\
    K_{ij} &=& K(|t_i - t_j|; \theta)
    \nonumber
  \end{eqnarray}
\end{frame}

\begin{frame}
  \frametitle{Gaussian processes}
  ~\hfill
  \includegraphics<1>[height=0.8\figureheight]{george/ind-results.png}
  \includegraphics<2>[height=0.8\figureheight]{george/gp-results.png}
\end{frame}

\begin{frame}
  \frametitle{Implementation details}
  \begin{itemize}
  \item There are many applied-math tricks for making things much
    faster than $N^{2.6}$.
  \item Use \project{george} {\footnotesize (Foreman-Mackey \etal)}.
  \item Marginalizing out the kernel hyperparameters $\theta$ is (in
    general) very expensive.
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Systematic noise model}
  \begin{itemize}
  \item Again, don't ``fit and subtract'' the stellar variability!
  \item Again, using $C + K$ in $\ln L$ is equivalent to fitting the
    stochastic variability simultaneously and then marginalizing it
    out!
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Kernel learning}
  \begin{itemize}
  \item Most uses of Gaussian Processes make use of kernel matrices
    with made-up kernels.
  \item \eg, $K(|t_i-t_j|;\theta) = a^2 \exp -\frac{|t_i-t_j|^2}{2\,\tau^2}$
  \item There are enormous families of kernels constructable from
    functions in the Fourier domain (eg, mixtures of Gaussians).
  \item With the data we have, we should be learning the kernels, not
    adopting convenient ones.
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Kernel learning {\footnotesize (Wilson \& Adams, 1302.4245)}}
  ~\hfill
  \includegraphics<1>[height=0.8\figureheight]{george/example_run_letters.jpg}
\end{frame}

\conclusions

\end{document}
